{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SR_GAN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["puyyh7v5NeED","dBfic5gNrcNa","Wgdggq-zr1m0","AHfYbtQxsE9I","kHk4AOu4sNiz","g5FD7TnZsabi"],"mount_file_id":"1kpMn84McxqBlYtbMQZ5js2PtdjsjBgYZ","authorship_tag":"ABX9TyOCDoeSjMSdW7XwIHxlqd0o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YAbDL1CKjtgN"},"source":["import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras import layers, Model\n","from keras.models import load_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-mNISQAlRqR"},"source":["# Defining Utility functions"]},{"cell_type":"code","metadata":{"id":"xHsd5lBFlKle"},"source":["Conv2D = layers.Conv2D\n","BatchNormalization = layers.BatchNormalization\n","PReLU = layers.PReLU\n","UpSampling2D = layers.UpSampling2D\n","Dense = layers.Dense\n","add = layers.add\n","LeakyReLU = layers.LeakyReLU\n","Input = layers.Input\n","Flatten = layers.Flatten\n","\n","\n","np.random.seed(10)\n","number_of_images =1000\n","hr_wdt, hr_hgt=256,256\n","downscale=4\n","input_dir=\"/content/drive/MyDrive/val2017\"\n","number_of_images = 1000\n","train_test_ratio = 0.8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFYbNgzZAd3m"},"source":["def load_path(path):\n","    directories = []\n","    if os.path.isdir(path):\n","        directories.append(path)\n","    for elem in os.listdir(path):\n","        if os.path.isdir(os.path.join(path,elem)):\n","            directories = directories + load_path(os.path.join(path,elem))\n","            directories.append(os.path.join(path,elem))\n","    return directories\n","    \n","def load_data_from_dirs(dirs, ext):\n","    files_hr, files_lr = [],[]\n","    file_names = []\n","    count = 0\n","    for d in dirs:\n","        for f in os.listdir(d): \n","            if f.endswith(ext):\n","                image = cv2.imread(d+\"/\"+f)\n","                h, w, c = image.shape\n","                if h>hr_hgt and w>hr_wdt and count<number_of_images:\n","                  image = cv2.resize(image, (hr_wdt, hr_hgt))                  \n","                  files_hr.append(image)\n","                  image = cv2.resize(image, (hr_wdt//downscale, hr_hgt//downscale))\n","                  files_lr.append(image)\n","                  file_names.append(os.path.join(d,f))\n","                  count = count + 1\n","                  print(count)\n","                  if(count==number_of_images):\n","                    break\n","    return files_hr, files_lr\n","    \n","def load_training_testing_data(directory, ext, number_of_images = 1000, train_test_ratio = 0.8):\n","\n","    number_of_train_images = int(number_of_images * train_test_ratio)\n","    \n","    files_hr, files_lr = load_data_from_dirs(load_path(directory), ext)\n","    \n","    if len(files_hr) < number_of_images:\n","        print(\"Number of image files are less then you specified\")\n","        print(\"Please reduce number of images to %d\" % len(files))\n","        sys.exit()\n","         \n","\n","    x_train_hr = files_hr[:number_of_train_images]\n","    x_train_lr = files_lr[:number_of_train_images]\n","    \n","    x_test_hr = files_hr[number_of_train_images:number_of_images]\n","    x_test_lr = files_lr[number_of_train_images:number_of_images]\n","    \n","    x_train_hr = np.array(x_train_hr) / 255\n","\n","    x_train_lr = np.array(x_train_lr) / 255\n","    \n","    x_test_hr = np.array(x_test_hr) / 255\n","    \n","    x_test_lr = np.array(x_test_lr) / 255\n","    \n","    return x_train_lr, x_train_hr, x_test_lr, x_test_hr\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hImvkznaXXX_"},"source":["train_lr, train_hr, test_lr, test_hr= load_training_testing_data(input_dir, '.jpg', number_of_images , train_test_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"puyyh7v5NeED"},"source":["#Defining the Architecture"]},{"cell_type":"markdown","metadata":{"id":"dBfic5gNrcNa"},"source":["###VGG for perceptual loss"]},{"cell_type":"code","metadata":{"id":"azbkWMb3Pxh6"},"source":["from keras.applications.vgg19 import VGG19\n","\n","def build_vgg():\n","    #vgg = VGG19(include_top=False, weights='imagenet', input_shape=hr_shape)\n","    #vgg.outputs = [vgg.layers[9].output]\n","\n","    #img = Input(shape=hr_shape)\n","\n","    #img_features = vgg(img)\n","\n","    #vgg19 = VGG19(include_top=False, weights='imagenet', input_shape=hr_shape)\n","    #img = Input(shape=hr_shape)\n","    #vgg19.trainable = False\n","    #for l in vgg19.layers:\n","    #    l.trainable = False\n","    #img_features = Model(inputs=img, outputs=vgg19.get_layer('block5_conv4').output)\n","\n","    #return Model(img, img_features)\n","\n","    vgg = VGG19(weights=\"imagenet\", input_shape= hr_shape, include_top = False)\n","    img = Input(shape=hr_shape)\n","    outputs = vgg.layers[9].output\n","    img_features = vgg(img)\n","    return Model(img, img_features)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wgdggq-zr1m0"},"source":["###Blocks used in Generator and Discriminator"]},{"cell_type":"code","metadata":{"id":"oxxWf4zolKiF"},"source":["def res_block(ip):\n","    \n","    res_model = Conv2D(64, (3,3), padding = \"same\")(ip)\n","    res_model = BatchNormalization(momentum = 0.5)(res_model)\n","    res_model = PReLU(shared_axes = [1,2])(res_model)\n","    \n","    res_model = Conv2D(64, (3,3), padding = \"same\")(res_model)\n","    res_model = BatchNormalization(momentum = 0.5)(res_model)\n","    \n","    return add([ip,res_model])\n","\n","def upscale_block(ip):\n","    \n","    up_model = Conv2D(256, (3,3), padding=\"same\")(ip)\n","    up_model = UpSampling2D( size = 2 )(up_model)\n","    up_model = PReLU(shared_axes=[1,2])(up_model)\n","    \n","    return up_model\n","\n","def discriminator_block(ip, filters, strides=1, bn=True):\n","    \n","    disc_model = Conv2D(filters, (3,3), strides = strides, padding=\"same\")(ip)\n","    disc_model = LeakyReLU( alpha=0.2 )(disc_model)\n","    if bn:\n","        disc_model = BatchNormalization( momentum=0.8 )(disc_model)\n","\n","    \n","    return disc_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AHfYbtQxsE9I"},"source":["###Generator"]},{"cell_type":"code","metadata":{"id":"pfxhnOanlKef"},"source":["def create_gen(gen_ip):\n","    layers = Conv2D(64, (9,9), padding=\"same\")(gen_ip)\n","    layers = PReLU(shared_axes=[1,2])(layers)\n","\n","    temp = layers\n","\n","    for i in range(num_res_block):\n","        layers = res_block(layers)\n","\n","    layers = Conv2D(64, (3,3), padding=\"same\")(layers)\n","    layers = BatchNormalization(momentum=0.5)(layers)\n","    layers = add([layers,temp])\n","\n","    layers = upscale_block(layers)\n","    layers = upscale_block(layers)\n","\n","    op = Conv2D(3, (9,9), padding=\"same\")(layers)\n","\n","    return Model(inputs=gen_ip, outputs=op)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kHk4AOu4sNiz"},"source":["###Discriminator"]},{"cell_type":"code","metadata":{"id":"VUfpSMJilKbU"},"source":["def create_disc(disc_ip):\n","\n","    df = 64\n","    \n","    d1 = discriminator_block(disc_ip, df, bn=False)\n","    d2 = discriminator_block(d1, df, strides=2)\n","    d3 = discriminator_block(d2, df*2)\n","    d4 = discriminator_block(d3, df*2, strides=2)\n","    d5 = discriminator_block(d4, df*4)\n","    d6 = discriminator_block(d5, df*4, strides=2)\n","    d7 = discriminator_block(d6, df*8)\n","    d8 = discriminator_block(d7, df*8, strides=2)\n","    \n","    d8_5 = Flatten()(d8)\n","    d9 = Dense(df*16)(d8_5)\n","    d10 = LeakyReLU(alpha=0.2)(d9)\n","    validity = Dense(1, activation='sigmoid')(d10)\n","\n","    return Model(disc_ip, validity)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g5FD7TnZsabi"},"source":["###GAN"]},{"cell_type":"code","metadata":{"id":"qjbffO-XlKYh"},"source":["def create_comb(gen_model, disc_model, vgg, lr_ip, hr_ip):\n","    gen_img = gen_model(lr_ip)\n","    \n","    gen_features = vgg(gen_img)\n","    \n","    disc_model.trainable = False\n","    validity = disc_model(gen_img)\n","    \n","    return Model(inputs=[lr_ip, hr_ip], outputs=[validity, gen_features])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G9V48MsnydW6"},"source":["#Training the network"]},{"cell_type":"markdown","metadata":{"id":"6F5Pnm4wKC8l"},"source":["###Loading data in batches"]},{"cell_type":"code","metadata":{"id":"OR4ucmxKlKEO"},"source":["batch_size = 16\n","epochs_pretraining_generator=300\n","epochs_training_gan=500\n","train_lr_batches = []\n","train_hr_batches = []\n","image_shape = (hr_wdt, hr_hgt,3)\n","for it in range(int(train_hr.shape[0] / batch_size)):\n","    start_idx = it * batch_size\n","    end_idx = start_idx + batch_size\n","    train_hr_batches.append(train_hr[start_idx:end_idx])\n","    train_lr_batches.append(train_lr[start_idx:end_idx])\n","\n","\n","num_res_block = 8\n","hr_shape = (train_hr.shape[1], train_hr.shape[2], train_hr.shape[3])\n","lr_shape = (train_lr.shape[1], train_lr.shape[2], train_lr.shape[3])\n","\n","lr_ip = Input(shape=lr_shape)\n","hr_ip = Input(shape=hr_shape)\n","\n","generator = create_gen(lr_ip)\n","generator.compile(loss=\"mse\", optimizer=\"adam\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b8Eeu7bM4MvV"},"source":["###Pre-training Generator\n","\n","* The GAN losses don't stabilize unless you first pretrain the generator. \n","* The network will still end up being able to improve image quality if you don't, but it will be only from the VGG loss and the GAN part will be basically useless. \n","* First we will train the generator to minimize the MSE between the training inputs and the training targets with no GAN.\n","* Once this model is trained, removed we will continue with training (using VGG perceptual loss + GAN loss and training the descriminator in the training loop)\n","For more info (https://www.fast.ai/2019/05/03/decrappify/)"]},{"cell_type":"code","metadata":{"id":"Jx5LbE8g3THM"},"source":["def pre_train_gen(epochs, batch_size, train_hr_batches, train_lr_batches,generator):\n","  for e in range(epochs):\n","    \n","    gen_losses = []\n","    for b in range(len(train_hr_batches)):\n","        lr_imgs = train_lr_batches[b]\n","        hr_imgs = train_hr_batches[b]\n","        \n","        d_loss_gen = generator.train_on_batch(lr_imgs, hr_imgs)\n","        \n","        gen_losses.append(d_loss_gen)\n","        \n","    gen_losses = np.array(gen_losses)\n","    \n","    gen_loss = np.sum(gen_losses, axis=0) / len(gen_losses)\n","    \n","    print(\"epoch:\", e+1 ,\"gen_loss:\", gen_loss)\n","\n","    if (e+1) % 5 == 0:\n","        generator.save(\"/content/output/gen/pre_trained_e_\"+ str(e+1) +\".h5\")\n","\n","pre_train_gen(epochs_pretraining_generator, batch_size, train_hr_batches, train_lr_batches,generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCdHy1LI9DA7"},"source":["###Loading the pretrained generator and training the complete GAN"]},{"cell_type":"code","metadata":{"id":"9o11MI4X6Qrj"},"source":["#Load the pretrained generator model\n","generator = load_model(\"/content/output/gen/pre_trained_e_45.h5\")\n","#generator = create_gen(lr_ip)\n","discriminator = create_disc(hr_ip)\n","discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n","\n","vgg = build_vgg()\n","vgg.trainable = False\n","\n","gan_model = create_comb(generator, discriminator, vgg, lr_ip, hr_ip)\n","gan_model.compile(loss=[\"binary_crossentropy\",\"mse\"], loss_weights=[1e-3, 1], optimizer=\"adam\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ztpNkGzzdcZ"},"source":["def train_gan(epochs, batch_size, train_hr_batches, train_lr_batches,generator,discriminator):\n","  for e in range(epochs):\n","    \n","    gen_label = np.zeros((batch_size, 1))\n","    real_label = np.ones((batch_size,1))\n","    g_losses = []\n","    d_losses = []\n","    for b in range(len(train_hr_batches)):\n","        lr_imgs = train_lr_batches[b]\n","        hr_imgs = train_hr_batches[b]\n","        \n","        gen_imgs = generator.predict_on_batch(lr_imgs)\n","        \n","        discriminator.trainable = True\n","        d_loss_gen = discriminator.train_on_batch(gen_imgs, gen_label)\n","        d_loss_real = discriminator.train_on_batch(hr_imgs, real_label)\n","        discriminator.trainable = False\n","        \n","        d_loss = 0.5 * np.add(d_loss_gen, d_loss_real) \n","        \n","        image_features = vgg.predict(hr_imgs)\n","\n","        \n","        g_loss, _, _ = gan_model.train_on_batch([lr_imgs, hr_imgs], [real_label, image_features])\n","        \n","        d_losses.append(d_loss)\n","        g_losses.append(g_loss)\n","        \n","    g_losses = np.array(g_losses)\n","    d_losses = np.array(d_losses)\n","    \n","    g_loss = np.sum(g_losses, axis=0) / len(g_losses)\n","    d_loss = np.sum(d_losses, axis=0) / len(d_losses)\n","    \n","    print(\"epoch:\", e+1 ,\"g_loss:\", g_loss, \"d_loss:\", d_loss)\n","\n","    if (e+1) % 20 == 0:\n","        discriminator.save_weights(\"/content/output/disc/e_\"+ str(e+1) +\".h5\")\n","        generator.save_weights(\"/content/output/gen/e_\"+ str(e+1) +\".h5\")\n","train_gan(epochs_training_gan, batch_size, train_hr_batches, train_lr_batches,generator,discriminator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvfyEMPVzdZB"},"source":["test_gen = generator.predict_on_batch(test_lr)\n","plt.imshow(test_gen[4])\n","plt.show()\n","plt.imshow(test_hr[4])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vEvCM5TEzdRH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWfrzG9szdH6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Y0CxqkczdBN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-i0mJJalJ39"},"source":[""],"execution_count":null,"outputs":[]}]}